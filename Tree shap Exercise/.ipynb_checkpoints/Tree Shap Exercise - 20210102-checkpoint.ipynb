{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\users\\inves\\appdata\\local\\programs\\python\\python38\\python.exe\n",
      "3.8.7 (tags/v3.8.7:6503f05, Dec 21 2020, 17:59:51) [MSC v.1928 64 bit (AMD64)]\n",
      "sys.version_info(major=3, minor=8, micro=7, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: this material is taken from Udacity's AI for Trading NanoDegree\n",
    "\n",
    "for the videos, please refer to Tree Shap Exercise Reference.txt in the same folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthony's Intro\n",
    "\n",
    "This is a continuation of the Shap Exercise, where we explore the complexity of actually calculating the Shap values for every feature and for every data point. Due to the complexity and the resource intensive nature (CPU and RAM) of this exercise, we try to simplify the process by fitting the data to the tree algorithm once, and use logic and resursive algo to derive shapley values. If you don't go through this exercise, what I want you to take away is shapley values can be calculated without using too much resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Shap\n",
    "\n",
    "To get a sense of the Shap library that we'll be using, let's implement the simple version of the Tree Shap algorithm.  This is based on Scott Lundberg's paper [Consistent Individualized Feature Attribution for Tree\n",
    "Ensembles](https://arxiv.org/pdf/1802.03888.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and re-using a single tree model\n",
    "\n",
    "You may notice that calculating Shap values for every feature, and for every individual data point, is very computationally expensive.  For example, we would be training multiple models just to calculate the importance of one feature.\n",
    "\n",
    "With decision trees, we can actually train a decision tree on all the features, and then re-use that single tree to calculate shapley values using subsets of that single tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./tree_shap_images/tree_shap_img_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some intuition, let’s look at a model that has just two features.  The tree splits on feature 1 first, and then on feature 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_02.png](./tree_shap_images/tree_shap_img_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to see how a model would perform if it only used the first feature, we could look at the subtree that consists of the top three nodes of this tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_03.png](./tree_shap_images/tree_shap_img_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if we wanted to see how a model would make predictions if it used only feature 2, we could look at the subtree containing the bottom three nodes, starting at the node that splits on feature 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example with 3 features\n",
    "Now let’s look at a tree that is trained on three features.  Let’s say it splits on feature 1, then on feature 2, then on feature 3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we simulate the prediction of a tree that was only trained on features 1 and 3, but not on feature 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, if we didn’t split on feature 2, that means that we would include the training samples in both the left and right sub-tree of that node when making a prediction.  This is how we can simulate that the tree never split on feature 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also think about how we handle the predictions when we do split on the feature.  If we split on feature 3, and the particular data point we’re making a prediction for ends up in the left child node, then we can use the prediction based on training samples in the left sub-tree, and ignore the training samples in the right sub-tree.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll walk through the algorithm to do this, and then you’ll get to practice this yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "\n",
    "Here’s the algorithm used to calculate the prediction of a tree, given a subset of features.  You can check out the paper [Consistent Individualized Feature Attribution for Tree Ensembles](https://arxiv.org/pdf/1802.03888.pdf), page 4 algorithm 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $G$ is a function that gets called recursively to walk down the tree starting at the root node.  $w$ is the weight given to the predictions of each node.  $v$ is the prediction of a leaf node.  $r_{a_j}$ and $r_{b_j}$ are the number of data points in the left and right child nodes of node $j$.  $r_j$ is the number of data points in node $j$.\n",
    "\n",
    "We can use this to walk through a decision tree that is trained on all features, and calculate the prediction of a tree that would have been created from a subset of the features.\n",
    "\n",
    "Let's look at specific parts of this algorithm in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leaf nodes\n",
    "Let's look at the the line that handles leaf nodes.  It takes the prediction of that leaf node and multiplies it by some weight.  The weight is determined by the proportion of training data points that end up reaching that leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ignoring a feature\n",
    "Next, let’s look at the case when the feature that’s used at a node is not within the subset of features that we want to split on.  In other words, we want to pretend that we didn’t train the model on this feature.  In that case, in order to pretend that we’re not splitting on that feature, we take the sum of the weighted predictions from both its left and right subtree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### including a feature\n",
    "\n",
    "Finally, for cases when the feature at that node is within the subset of features that we want to use, then we can follow just the left subtree or just the right subtree, whichever path that the input data gets assigned to by the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement it in code!\n",
    "You’ll get to practice this algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.14.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.2MB 2.7MB/s eta 0:00:01   38% |████████████▎                   | 4.7MB 25.8MB/s eta 0:00:01    49% |███████████████▊                | 6.0MB 29.2MB/s eta 0:00:01\n",
      "\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "Successfully installed numpy-1.14.5\n",
      "Requirement already satisfied: scikit-learn==0.19.1 in /opt/conda/lib/python3.6/site-packages (0.19.1)\n",
      "Collecting graphviz==0.9\n",
      "  Downloading https://files.pythonhosted.org/packages/47/87/313cd4ea4f75472826acb74c57f94fc83e04ba93e4ccf35656f6b7f502e2/graphviz-0.9-py2.py3-none-any.whl\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.9\n",
      "Collecting shap==0.25.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/85/196abf4c178446e077237e4c649e93637393dac24ab7f760143c4ef1548e/shap-0.25.2.tar.gz (197kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 8.3MB/s ta 0:00:01   10% |███▎                            | 20kB 12.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (1.14.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (0.19.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (2.1.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (0.23.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (4.11.2)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (6.5.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2) (1.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2) (2.6.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2) (2017.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib->shap==0.25.2) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2) (2.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (0.7.4)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (2.2.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (4.3.1)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (38.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (1.0.15)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (0.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (4.3.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (0.10.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (4.0.11)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->shap==0.25.2) (0.5.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython->shap==0.25.2) (0.1.7)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2->ipython->shap==0.25.2) (0.2.0)\n",
      "Building wheels for collected packages: shap\n",
      "  Running setup.py bdist_wheel for shap ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3f/cb/58/2482f534eb1c1f9ac93653887aee5b6ea83718ef041c1310bd\n",
      "Successfully built shap\n",
      "Installing collected packages: shap\n",
      "Successfully installed shap-0.25.2\n"
     ]
    }
   ],
   "source": [
    "# run this only if you lack these libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.14.5\n",
    "!{sys.executable} -m pip install scikit-learn==0.19.1\n",
    "!{sys.executable} -m pip install graphviz==0.9\n",
    "!{sys.executable} -m pip install shap==0.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "import shap\n",
    "import numpy as np\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate sample data\n",
    "\n",
    "Feature 0 and feature 1 form the AND operator, and feature 2 does not contribute to the prediction of the label, because it's always zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some versions of graphviz doesn't work, this is a workaround, downloading graphviz-2.44.1-win32.zip from https://www2.graphviz.org/Packages/stable/windows/10/msbuild/Release/Win32/\n",
    "#unzip, and place in a desired folder.\n",
    "\n",
    "import os\n",
    "graphviz_folder = 'C:\\\\Users\\\\inves\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\Scripts\\\\graphviz-2.44.1-win32\\\\Graphviz\\\\bin\\\\'\n",
    "os.environ[\"PATH\"] += os.pathsep + graphviz_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND case (features 0 and 1)\n",
    "N = 100\n",
    "M = 3\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:1 * N//4, 1] = 1\n",
    "X[:N//2, 0] = 1\n",
    "X[N//2:3 * N//4, 1] = 1\n",
    "y[:1 * N//4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 1. 0.], targets: 1.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [1. 0. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 1. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n",
      "features: [0. 0. 0.], targets: 0.0\n"
     ]
    }
   ],
   "source": [
    "# better way to observe data\n",
    "for features, target in zip(X, y):\n",
    "    print('features: {}, targets: {}'.format(features, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.44.1 (20200629.0800)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"292pt\" height=\"261pt\"\r\n",
       " viewBox=\"0.00 0.00 292.00 261.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 257)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-257 288,-257 288,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\">\r\n",
       "<title>0</title>\r\n",
       "<path fill=\"#f8e0ce\" stroke=\"black\" d=\"M155,-253C155,-253 68,-253 68,-253 62,-253 56,-247 56,-241 56,-241 56,-201 56,-201 56,-195 62,-189 68,-189 68,-189 155,-189 155,-189 161,-189 167,-195 167,-201 167,-201 167,-241 167,-241 167,-247 161,-253 155,-253\"/>\r\n",
       "<text text-anchor=\"start\" x=\"85\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\r\n",
       "<text text-anchor=\"start\" x=\"95\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">1</text>\r\n",
       "<text text-anchor=\"start\" x=\"102\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"71\" y=\"-224.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.188</text>\r\n",
       "<text text-anchor=\"start\" x=\"64\" y=\"-210.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\r\n",
       "<text text-anchor=\"start\" x=\"72.5\" y=\"-196.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.25</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\">\r\n",
       "<title>1</title>\r\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M91,-147.5C91,-147.5 12,-147.5 12,-147.5 6,-147.5 0,-141.5 0,-135.5 0,-135.5 0,-106.5 0,-106.5 0,-100.5 6,-94.5 12,-94.5 12,-94.5 91,-94.5 91,-94.5 97,-94.5 103,-100.5 103,-106.5 103,-106.5 103,-135.5 103,-135.5 103,-141.5 97,-147.5 91,-147.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"19.5\" y=\"-132.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-117.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\r\n",
       "<text text-anchor=\"start\" x=\"16.5\" y=\"-102.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.0</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\">\r\n",
       "<title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92.52,-188.99C86.13,-178.57 78.98,-166.88 72.45,-156.22\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"75.35,-154.26 67.15,-147.56 69.38,-157.91 75.35,-154.26\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"61.3\" y=\"-168.16\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\">\r\n",
       "<title>2</title>\r\n",
       "<path fill=\"#f2c09c\" stroke=\"black\" d=\"M212,-153C212,-153 133,-153 133,-153 127,-153 121,-147 121,-141 121,-141 121,-101 121,-101 121,-95 127,-89 133,-89 133,-89 212,-89 212,-89 218,-89 224,-95 224,-101 224,-101 224,-141 224,-141 224,-147 218,-153 212,-153\"/>\r\n",
       "<text text-anchor=\"start\" x=\"146\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\r\n",
       "<text text-anchor=\"start\" x=\"156\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">0</text>\r\n",
       "<text text-anchor=\"start\" x=\"163\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"136\" y=\"-124.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.25</text>\r\n",
       "<text text-anchor=\"start\" x=\"129\" y=\"-110.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\r\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\">\r\n",
       "<title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130.8,-188.99C136.2,-180.32 142.14,-170.78 147.8,-161.68\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"150.8,-163.48 153.12,-153.14 144.86,-159.78 150.8,-163.48\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"158.78\" y=\"-173.79\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\">\r\n",
       "<title>3</title>\r\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M151,-53C151,-53 72,-53 72,-53 66,-53 60,-47 60,-41 60,-41 60,-12 60,-12 60,-6 66,0 72,0 72,0 151,0 151,0 157,0 163,-6 163,-12 163,-12 163,-41 163,-41 163,-47 157,-53 151,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"79.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\r\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.0</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\">\r\n",
       "<title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.02,-88.94C146.2,-80.11 139.82,-70.45 133.89,-61.46\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.8,-59.51 128.38,-53.09 130.96,-63.37 136.8,-59.51\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\">\r\n",
       "<title>4</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M272,-53C272,-53 193,-53 193,-53 187,-53 181,-47 181,-41 181,-41 181,-12 181,-12 181,-6 187,0 193,0 193,0 272,0 272,0 278,0 284,-6 284,-12 284,-12 284,-41 284,-41 284,-47 278,-53 272,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"200.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"189\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\r\n",
       "<text text-anchor=\"start\" x=\"197.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 1.0</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\">\r\n",
       "<title>2&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.65,-88.94C198.31,-80.2 204.51,-70.65 210.29,-61.74\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"213.39,-63.39 215.9,-53.09 207.52,-59.58 213.39,-63.39\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x295277ba7c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model = sklearn.tree.DecisionTreeRegressor(random_state = 0)\n",
    "model.fit(X, y)\n",
    "\n",
    "# draw model\n",
    "dot_data = sklearn.tree.export_graphviz(model, out_file = None, filled = True, rounded = True, special_characters = True)\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree attributes\n",
    "\n",
    "[sklearn.tree.tree._tree](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx)\n",
    "\n",
    "```\n",
    "The binary tree is represented as a number of parallel arrays. The i-th\n",
    "    element of each array holds information about the node `i`. Node 0 is the\n",
    "    tree's root.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree0 = model.tree_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  left and right child nodes\n",
    "```\n",
    "children_left : array of int, shape [node_count]\n",
    "        children_left[i] holds the node id of the left child of node i.\n",
    "        For leaves, children_left[i] == TREE_LEAF. Otherwise,\n",
    "        children_left[i] > i. This child handles the case where\n",
    "        X[:, feature[i]] <= threshold[i].\n",
    "    children_right : array of int, shape [node_count]\n",
    "        children_right[i] holds the node id of the right child of node i.\n",
    "        For leaves, children_right[i] == TREE_LEAF. Otherwise,\n",
    "        children_right[i] > i. This child handles the case where\n",
    "        X[:, feature[i]] > threshold[i].\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.children_left: [ 1 -1  3 -1 -1]\n",
      "tree0.children_right: [ 2 -1  4 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.children_left: {tree0.children_left}\")\n",
    "print(f\"tree0.children_right: {tree0.children_right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features\n",
    "```\n",
    "    feature : array of int, shape [node_count]\n",
    "        feature[i] holds the feature to split on, for the internal node i.\n",
    "    threshold : array of double, shape [node_count]\n",
    "        threshold[i] holds the threshold for the internal node i.\n",
    "    value : array of double, shape [node_count, n_outputs, max_n_classes]\n",
    "        Contains the constant prediction value of each node.\n",
    "    impurity : array of double, shape [node_count]\n",
    "        impurity[i] holds the impurity (i.e., the value of the splitting\n",
    "        criterion) at node i.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.feature: [ 1 -2  0 -2 -2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.feature: {tree0.feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Node 0, feature 1 is used to split the data . For Node 2, feature 0 is used for splitting.  For the other nodes (1, 3, 4), there are no features used for splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.threshold: [ 0.5 -2.   0.5 -2.  -2. ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.threshold: {tree0.threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold divides the data points using the chosen feature.  <= 0.5 and the data go to the left child; > .5 and the data go in the right child.  The -2 is for nodes that don't split on any feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.value : \n",
      "[[[0.25]]\n",
      "\n",
      " [[0.  ]]\n",
      "\n",
      " [[0.5 ]]\n",
      "\n",
      " [[0.  ]]\n",
      "\n",
      " [[1.  ]]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.value : \\n{tree0.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value` is the average prediction for each node.  Node 0 predicts 0.25 on average.  Node 2 predicts 0.5 on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### node samples\n",
    "```\n",
    "n_node_samples : array of int, shape [node_count]\n",
    "        n_node_samples[i] holds the number of training samples reaching node i.\n",
    "\n",
    "weighted_n_node_samples : array of int, shape [node_count]\n",
    "        weighted_n_node_samples[i] holds the weighted number of training samples\n",
    "        reaching node i.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.n_node_samples : [100  50  50  25  25]\n",
      "tree0.weighted_n_node_samples : [100.  50.  50.  25.  25.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.n_node_samples : {tree0.n_node_samples}\")\n",
    "print(f\"tree0.weighted_n_node_samples : {tree0.weighted_n_node_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n_node_samples counts how many data points from the parent gets passed to each child node.  `weighted_n_node_samples` is the same in this case, because it's a single decision tree.  If this were a tree within a random forest, 2/3 of the training data may be sampled and used to train a tree.  The `weighted_n_node_samples` would then be re-scaled to equal the total sample size.  We can use either in the calculations we'll do below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tree attributes section, here are what we care about:\n",
    "note that the binary tree is represented by an array. the ith element of each array holds info about the node i. Node 0 is the root node.\n",
    "1. children_left, children_right: left and right nodes of the root\n",
    "2. feature: which feature is used at which node. value is the feature, position is the node\n",
    "3. threshold: what value we are using to divide the datapoints. value is the threshold, poisition is the node.\n",
    "4. value: average prediction for each node. value is the value, position is the node.\n",
    "5. n_node_samples, weighted_n_node_samples: number of samples at each node. value is the # samples, position is the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz\n",
    "What proportion of samples went to the left child and right child of the root node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion of samples in left child of root node 0.5\n",
      "proportion of samples in left child of root node 0.5\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "proportion_in_left_child = tree0.weighted_n_node_samples[1] / tree0.weighted_n_node_samples[0] # ...\n",
    "proportion_in_right_child = tree0.weighted_n_node_samples[2] / tree0.weighted_n_node_samples[0] # ...\n",
    "print(f\"proportion of samples in left child of root node {proportion_in_left_child}\")\n",
    "print(f\"proportion of samples in left child of root node {proportion_in_right_child}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap with Tree class\n",
    "\n",
    "To make the tree object easier to work with, we'll wrap it inside our custom Tree class.  Please complete the functions within the Tree class below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Challenge:** Try implementing your own wrapper class for the tree object.\n",
    "\n",
    "Think about attributes that you may need in order to implement algorithm 1.  For example, how do we know when a node is an internal or leaf node?  What fraction of samples are in the left child relative to its parent node?  On which node is each feature split on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Challenge: try implementing your own wrapper class\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you prefer some starter code:** You can also use the starter code below if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_NODE = -1\n",
    "NO_FEATURE = -2\n",
    "\n",
    "class Tree:\n",
    "    \n",
    "    def __init__(self, tree):\n",
    "        if str(type(tree)).endswith(\"'sklearn.tree._tree.Tree'>\"):\n",
    "            self.weight = 1 \n",
    "            self.children_left = tree.children_left\n",
    "            self.children_right = tree.children_right\n",
    "            self.features = tree.feature\n",
    "            self.thresholds = tree.threshold\n",
    "            self.values = tree.value[:, 0, 0] # tree.value is n by 1 by 1; get the n prediction values (there are n nodes in tree)\n",
    "            self.n_node_samples = tree.n_node_samples # actual number of nodes\n",
    "            self.node_sample_weight = tree.weighted_n_node_samples #rescaled number of nodes\n",
    "\n",
    "    def is_internal(self, i):\n",
    "        return (self.children_left[i] != NO_NODE or self.children_right[i] != NO_NODE)\n",
    "\n",
    "    def is_leaf(self, i):\n",
    "        # TODO\n",
    "        return (self.children_left[i] == NO_NODE and self.children_right[i] == NO_NODE)\n",
    "            \n",
    "    def left_child(self, i):\n",
    "        return self.children_left[i]\n",
    "\n",
    "    def right_child(self, i):\n",
    "        # TODO\n",
    "        return self.children_right[i]\n",
    "\n",
    "    def proportion_of_samples_in_left_child(self, i):\n",
    "        # TODO\n",
    "        return self.n_node_samples[self.left_child(i)] / self.n_node_samples[i]\n",
    "    \n",
    "    def proportion_of_samples_in_right_child(self,i):\n",
    "        # TODO\n",
    "        return self.n_node_samples[self.right_child(i)] / self.n_node_samples[i]\n",
    "\n",
    "    def node_prediction(self, i):\n",
    "        return self.values[i]\n",
    "\n",
    "    def feature_that_split_node_i(self, i):\n",
    "        return self.features[i]\n",
    "\n",
    "    def threshold_at_node_i(self, i):\n",
    "        return self.thresholds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_wrap = Tree(tree0) #so this Tree Wrap takes in model.tree_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the prediction of a tree model, given a subset of features\n",
    "\n",
    "We'll implement algorithm 1 of Scott Lundberg's paper.  This is a way to use a single trained tree to estimate predictions of other trees that would be trained on a subset of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus challenge:** Try implementing this function completely by yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bonus challenge: implement f_given_S on your own!\"\"\"\n",
    "def f_given_S(tree, S, x):\n",
    "    \"\"\"\n",
    "    tree: the custom Tree class\n",
    "    S: set of integers reprenting features that are used to train the model.\n",
    "    x: sample observation on which to calculate the prediction of the model.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you prefer some starter code:** You can also use the starter code below to implement the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can use this starter code if you get stuck while implementing the algorithm on your own\n",
    "\"\"\"\n",
    "def f_given_S(tree, S, x):\n",
    "    \"\"\"\n",
    "    tree: the custom Tree class\n",
    "    S: set of integers reprenting features that are used to train the model.\n",
    "    x: sample observation on which to calculate the prediction of the model.\n",
    "    \"\"\"\n",
    "    # the root node is at index 0 in the list\n",
    "    starting_node = 0\n",
    "    # When starting at the root node, the weight assigned is 1 (100%).\n",
    "    starting_weight = 1\n",
    "\n",
    "    def traverse_tree(node_i, weight):\n",
    "        \"\"\"\n",
    "        nested function that will be called recursively\n",
    "        \"\"\"\n",
    "        if tree.is_leaf(node_i):\n",
    "            # TODO: multiply the weight times the node prediction\n",
    "            return weight * tree.node_prediction(node_i)\n",
    "        \n",
    "        else: # is internal node\n",
    "            feature_index = tree.feature_that_split_node_i(node_i)\n",
    "            feature_value = x[feature_index]\n",
    "            threshold = tree.threshold_at_node_i(node_i)\n",
    "            left_child = tree.left_child(node_i)\n",
    "            right_child = tree.right_child(node_i)\n",
    "            if feature_index in S: #if the feature is in the subset S\n",
    "                if feature_value <= threshold:\n",
    "                    # TODO: recursively traverse the left subtree\n",
    "                    return traverse_tree(left_child, weight)\n",
    "                else:\n",
    "                    # TODO: recursively traverse the right subtree\n",
    "                    return traverse_tree(right_child, weight)\n",
    "                \n",
    "            else: #feature is not in the subset S\n",
    "                # TODO: traverse the left sub-tree,\n",
    "                # and update the weight to be the current weight times the proportion of samples in the left child node\n",
    "                left_subtree = traverse_tree(left_child, weight * tree.proportion_of_samples_in_left_child(node_i))\n",
    "                \n",
    "                # TODO: traverse the right sub-tree,\n",
    "                # and update the weight to be the current weight times the proportion of samples in the left child node\n",
    "                right_subtree = traverse_tree(right_child, weight * tree.proportion_of_samples_in_right_child(node_i))\n",
    "                \n",
    "                # TODO: return the sum of both sub-trees\n",
    "                return left_subtree + right_subtree\n",
    "    \n",
    "    # start traversing the tree\n",
    "    return traverse_tree(starting_node,starting_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_values = np.array([1, 1, 1])\n",
    "S = set([2]) # if you input only feature 2, expect 0.25, a simple average of all the labels, as feature 2 doesn't do anything\n",
    "f_given_S(tree_wrap, S, sample_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try empty feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = set([]) #for empty set, expect output to be 0.25\n",
    "\n",
    "f_given_S(tree_wrap, S, sample_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate weight on marginal contribution\n",
    "\n",
    "We'll calculate the weight placed on the marginal contribution of the feature:\n",
    "$ \\frac{|S|! (M - |S| -1 )!}{M!}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "def weight_on_marginal_contribution(size_S, M):\n",
    "    \"\"\"\n",
    "    size_S: number of features in set S\n",
    "    M: numer of total features\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return factorial(size_S) * factorial(M - size_S - 1) / factorial(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate marginal contribution for a single feature \n",
    "\n",
    "$ f(S \\cup i) - f(S)$  \n",
    "\n",
    "Fill in the function that takes in the custom Tree object, a sample data point, a list containing the set of features in set S (excluding feature \"i\"), and also the feature for which we want to calculate the marginal contribution.  Keep in mind that set S excludes feature \"i\".\n",
    "\n",
    "**Hint:** The python `set` class has the member function `.add`.  \n",
    "Note that you may need to use the `.copy` function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_contribution_of_feature(tree, x, S, feature_i):\n",
    "    \"\"\"\n",
    "    tree: the custom Tree object that wraps the sklearn tree_ object\n",
    "    x: a sample observation that contains all features\n",
    "    S: a list of integers, specifying the features in subset S, excluding feature i.\n",
    "    feature_i: an integer specifying the feature for which we're calculating the marginal contribution.\n",
    "    \"\"\"\n",
    "    # TODO: create the union of S and i\n",
    "    S_union_i = S.copy()\n",
    "    S_union_i.add(feature_i)\n",
    "    # TODO: return the difference in prediction with feature \"i\" and without feature \"i\"\n",
    "    return f_given_S(tree, S_union_i, x) - f_given_S(tree, S, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try it out\n",
    "\n",
    "We'll try out the `marginal_contribution_of_feature` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_i = 0 # index of feature for which we want to calculate its marginal contribution\n",
    "S = set([1]) # Set that excludes feature i\n",
    "x = X[0] # grab one data point to calculate marginal contribution on\n",
    "marginal_contribution_of_feature(tree_wrap, x, S, feature_i) # we expect 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal contribution of feature 0 is 0.5.  This means that the prediction of the model when feature 0 is present is 0.5 greater than the model's prediction when it only has feature 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate all subsets\n",
    "\n",
    "Fill in a function that generates all possible subsets S.  \n",
    "We'll use `itertools.combinations`, which takes in a list, and also the size of each subset.  It returns an iterable object that contains tuples of all the combinations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(1, 3)\n",
      "(1, 4)\n",
      "(2, 3)\n",
      "(2, 4)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# try out the combinations function\n",
    "tmp_combo = combinations([1, 2, 3, 4], 2)\n",
    "for subset in tmp_combo:\n",
    "    print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill in the function generate_all_subsets\n",
    "\n",
    "Keep in mind that since the iterable returned by `combinations` holds tuples, we can create sets out of the tuples by using `set(the_tuple_object)`.  We'll store the S sets as `set` types, since we defined the `f_given_S` function to take S as a type `set`.\n",
    "\n",
    "Remember to also include the empty set.  We can do this with `set([None])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus challenge:** Try implementing this function on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Try implementing on your own!\n",
    "\"\"\"\n",
    "def generate_all_subsets(all_features):\n",
    "    \"\"\"\n",
    "    all_features: set of integers representing all the features\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you prefer some starter code:** You can also use the starter code if you get stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Starter code version\n",
    "\"\"\"\n",
    "\n",
    "def generate_all_subsets(all_features):\n",
    "    \"\"\"\n",
    "    all_features: set of integers representing all the features\n",
    "    \"\"\"\n",
    "    sets_l = []\n",
    "    for size in range(1, len(all_features) + 1):\n",
    "        # TODO: create a combinations iterable\n",
    "        combo = combinations(all_features, size)\n",
    "        # TODO: loop thru the combo iterable and append sets to the sets_l list\n",
    "        for subset in combo:\n",
    "            sets_l.append(set(subset))\n",
    "            \n",
    "    # TODO: also include the empty set\n",
    "    sets_l.append(set([None]))\n",
    "    return sets_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0}, {1}, {2}, {0, 1}, {0, 2}, {1, 2}, {0, 1, 2}, {None}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = set([0, 1, 2])\n",
    "generate_all_subsets(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate shapley value for one feature\n",
    "\n",
    "Implement a function that calculates the shapley value for a single feature, by iterating across all subsets S.\n",
    "\n",
    "$ \\phi_{i} = \\sum_{S \\subseteq M \\setminus i} \\frac{|S|! (|M| - |S| -1 )!}{|M|!} [f(S \\cup i) - f(S)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus challenge:** Try implementing the function yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_feature_i(tree,x,feature_i):\n",
    "    \"\"\"\n",
    "    tree: the custom Tree object that wraps the tree_ from sklearn.\n",
    "    x: a sample data point\n",
    "    i: the feature for which we want to calculate its shapley value.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you prefer starter code:** You can also fill in the starter code below if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_feature_i(tree, x, feature_i):\n",
    "    \"\"\"\n",
    "    tree: the custom Tree object that wraps the tree_ from sklearn.\n",
    "    x: a sample data point\n",
    "    i: the feature for which we want to calculate its shapley value.\n",
    "    \"\"\"\n",
    "    all_features = set(np.arange(0, x.shape[0]))\n",
    "    all_features_minus_i = all_features.copy()\n",
    "    all_features_minus_i.remove(feature_i) #remove feature \"i\"\n",
    "\n",
    "    # TODO: generate all subsets S\n",
    "    S_list = generate_all_subsets(all_features_minus_i) # ...\n",
    "    phi = 0 # initialize\n",
    "    num_features_total = len(all_features)\n",
    "    \n",
    "    # iterate thru S_list\n",
    "    for S in S_list:\n",
    "        # TODO: calculate the number of features stored in S\n",
    "        # Handle the special case where S contains None, because 'None' shouldn't count as a feature\n",
    "        if None in S:\n",
    "            num_features_in_S = len(S) - 1\n",
    "        else:\n",
    "            num_features_in_S = len(S)\n",
    "            \n",
    "        # TODO: increment phi by the weight on the marginal contribution * marginal contribution of \"i\"\n",
    "        phi += weight_on_marginal_contribution(num_features_in_S, num_features_total) * marginal_contribution_of_feature(tree, x, S, feature_i)\n",
    "    \n",
    "    return np.round(phi, decimals = 3)\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[0]\n",
    "shap_feature_i(tree_wrap, x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate feature importance of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_tree_explainer(tree_wrap, x):\n",
    "    shap_l = []\n",
    "    for i, v in enumerate(x):\n",
    "        shap_l.append(shap_feature_i(tree_wrap, x, i))\n",
    "    \n",
    "    return np.array(shap_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take an sklearn tree model and calculate feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_tree_model_explainer(tree_model, x):\n",
    "    sklearn_tree = tree_model.tree_\n",
    "    tree_wrap = Tree(tree_model.tree_)\n",
    "    \n",
    "    return shap_tree_explainer(tree_wrap, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive Feature Attribution\n",
    "\n",
    "Additive feature attribution methods are simple models that are used to explain complex models.  You can see the formula in the same paper on page 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./tree_shap_images/tree_shap_img_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Think of our tree model as the complex model that we wish to explain with a simple, linear model.  The above formula is saying that we can take a single data point with the three features, and the complex model makes a prediction.  We can divide up that prediction among the three features, based on how important those features are to the complex model's prediction, and also based on whether the feature values push the prediction in the positive or negative direction.\n",
    "\n",
    "This is related to the ideas of coalition game theory.  Imagine a team of basketball players scores 100 points in a game. We are trying to attribute part of the final score to each member of the team, based on their contributions, or \"importance.\"\n",
    "\n",
    "When the contributions of each feature are added up to equal the complex model's prediction, this linear combination of contributions is the simple linear model that is being used to explain the complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Let's say that we've trained a complex model on 3 features.  If it's given no inputs to make a prediction, then its prediction would be the equal weighted average of all its training samples.  \n",
    "Let's say that equal weighted average of the training labels is **100**.  In other words, if a model is given no features and asked to make a prediction, it would predict 100, which is the expected value based on the training labels.\n",
    "\n",
    "Now, let's say we give the complex model a single sample observation, with all three features, and the complex model gives a prediction of **200**.  \n",
    "\n",
    "The additive feature attribution model may assign feature importances to the three features like this:  \n",
    "feature 0: +50  \n",
    "feature 1: +90  \n",
    "feature 2: -40  \n",
    "\n",
    "So this is saying that feature 0 pushed the complex model's prediction up by 50, feature 1 pushed the complex model's prediction up by 90, and feature 2 pushed the model's prediction down by 40.  The end result was to go from the expected value of 100 to the prediction of 200.\n",
    "\n",
    "The shapley values that we just calculated are these values that push the model's prediction from the average of the training labels to the model's final prediction.  When we add up the shapley values for all the features, they add up to the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare this implementation with shap library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my shap function: [-0.125 -0.125  0.   ]\n",
      "shap library:     [-0.125 -0.125  0.   ]\n",
      "expected value (average of labels in y) 0.25\n",
      "sum of shapley values for all features: -0.25\n",
      "model prediction [0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 0, 0])\n",
    "shap_values = shap_tree_model_explainer(model, x)\n",
    "expected_value = np.mean(y)\n",
    "print(f\"my shap function: {shap_values}\")\n",
    "print(f\"shap library:     {shap.TreeExplainer(model).shap_values(x)}\")\n",
    "print(f\"expected value (average of labels in y) {expected_value}\")\n",
    "print(f\"sum of shapley values for all features: {np.sum(shap_values)}\")\n",
    "print(f\"model prediction {model.predict(x.reshape(1, -1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz\n",
    "\n",
    "How do you interpret the shapley values of each feature when features 0,1 and 2 are all 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "when the model have nothing to train on, it's best guess is the average of the labels. when shap received the information that the features should push the value down by -0.25 (total), then average target value + sum of shapley values = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my shap function: [ 0.125 -0.375  0.   ]\n",
      "shap library:     [ 0.125 -0.375  0.   ]\n",
      "expected value (average of labels in y) 0.25\n",
      "sum of shapley values for all features: -0.25\n",
      "model prediction [0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 0, 0])\n",
    "shap_values = shap_tree_model_explainer(model, x)\n",
    "print(f\"my shap function: {shap_values}\")\n",
    "print(f\"shap library:     {shap.TreeExplainer(model).shap_values(x)}\")\n",
    "print(f\"expected value (average of labels in y) {expected_value}\")\n",
    "print(f\"sum of shapley values for all features: {np.sum(shap_values)}\")\n",
    "print(f\"model prediction {model.predict(x.reshape(1,-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz\n",
    "\n",
    "How do you interpret the shapley values of each feature when feature 0 is 1 and the other features are 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Even though the different features pushes the prediction in different directions, we still have a prediction of 0. This can be interpreted as: in an AND statement, the False often pulls the direction much more than the True. In test 2, False has shap value of -0.375, which has a greater magnitude than the True, which has shap value of 0.125."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my shap function: [0.375 0.375 0.   ]\n",
      "shap library:     [0.375 0.375 0.   ]\n",
      "expected value (average of labels in y) 0.25\n",
      "sum of shapley values for all features: 0.75\n",
      "model prediction [1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 1, 0])\n",
    "shap_values = shap_tree_model_explainer(model, x)\n",
    "print(f\"my shap function: {shap_values}\")\n",
    "print(f\"shap library:     {shap.TreeExplainer(model).shap_values(x)}\")\n",
    "print(f\"expected value (average of labels in y) {expected_value}\")\n",
    "print(f\"sum of shapley values for all features: {np.sum(shap_values)}\")\n",
    "print(f\"model prediction {model.predict(x.reshape(1,-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz\n",
    "\n",
    "How do we interpret the shapley values when feature 0 and 1 are both 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Features 0 and 1 are contributing equally to the final prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "[solution notebook](tree_shap_solution.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
